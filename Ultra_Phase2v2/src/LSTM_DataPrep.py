
#from __future__ import print_function
from six.moves import xrange
import six.moves.cPickle as pickle

import gzip
import os
import numpy
import theano


'''
####################################################################################
'''

def prepare_data(seqs, labels, maxlen=None):
    """
    Create the matrices from the datasets.

    This pad each sequence to the same lenght: 
    the lenght of the longuest sequence or maxlen.
    if maxlen is set, we will cut all sequence to this maximum lenght.

    This swap the axis!

    """
    # seqs or x: a list of sentences
    lengths = [len(s) for s in seqs]
    # use maxlen to truncate data
    if maxlen is not None:
        new_seqs = []
        new_labels = []
        new_lengths = []
        for l, s, y in zip(lengths, seqs, labels):
            if l < maxlen:
                new_seqs.append(s) # X
                new_labels.append(y) # Y
                new_lengths.append(l)
        lengths = new_lengths
        labels = new_labels # Y
        seqs = new_seqs # X

        if len(lengths) < 1:
            return None, None, None

    n_samples = len(seqs) # X
    maxlen = numpy.max(lengths)
    # X matrix, maxlen rows, n_samples columns
    # each sentence is on a column
    x = numpy.zeros((maxlen, n_samples)).astype('int64')
    x_mask = numpy.zeros((maxlen, n_samples)).astype(theano.config.floatX)
    # load data into X, data as values of each word from dict of word2index
    # mark x_mask with '1' on cells with words, '0' on cells without
    for idx, s in enumerate(seqs):
        x[:lengths[idx], idx] = s
        x_mask[:lengths[idx], idx] = 1.
    # lables as Y
    return x, x_mask, labels

'''
####################################################################################
'''

def get_dataset_file(dataset, default_dataset, path_dataset):
    '''
    Look for it as if it was a full path, 
    if not, try local file,
    if not try in the data directory.
    
    parameters:
    ---
    dataset: name of the data set; name of both .pkl files
    path_dataset: path to the data set under folder Data
                e.g. "../Data/DataSet_Tokenize"

    '''
    data_dir, data_file = os.path.split(dataset)
    if data_dir == "" and not os.path.isfile(dataset):
        # Check if dataset is in the data directory.
        new_path = os.path.join(
            os.path.split(__file__)[0],
            path_dataset,
            dataset
        )
        if os.path.isfile(new_path) or data_file == default_dataset:
            dataset = new_path
    
    return dataset

'''
####################################################################################
'''

def load_data(dataset, path_dataset, n_words=60000, valid_portion=0.1, maxlen=None,
              sort_by_len=True):
    '''
    Loads the dataset
    '''
    #################################################################
    # Load the dataset
    dataset = get_dataset_file(dataset=dataset, default_dataset="tweetText_tagScore.pkl", 
                               path_dataset= path_dataset) 
    print "load data: ", dataset

    if dataset.endswith(".gz"):
        f = gzip.open(dataset, 'rb')
    else:
        f = open(dataset, 'rb')

    # tweetText_tagScore.pkl is generated by 2 pkl.dump()
    train_set = pickle.load(f) # train_set is tuple, (X, Y)
    test_set = pickle.load(f) 

    f.close()
    #################################################################   

    # limit sentence length by maxlen
    if maxlen:
        new_train_set_x = []
        new_train_set_y = []
        for x, y in zip(train_set[0], train_set[1]):
            if len(x) < maxlen:
                new_train_set_x.append(x)
                new_train_set_y.append(y)
        train_set = (new_train_set_x, new_train_set_y)
        del new_train_set_x, new_train_set_y

    # split training set into validation set
    train_set_x, train_set_y = train_set
    n_samples = len(train_set_x)
    sidx = numpy.random.permutation(n_samples) # draw validation set randomly
    n_train = int(numpy.round(n_samples * (1. - valid_portion)))
    
    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]
    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]
    
    train_set_x = [train_set_x[s] for s in sidx[:n_train]]
    train_set_y = [train_set_y[s] for s in sidx[:n_train]]

    train_set = (train_set_x, train_set_y)
    valid_set = (valid_set_x, valid_set_y)

    #################################################################
    def remove_unk(x):
        # flag words by whether their frequency is above the threshold or not
        # threshod set by n_words and the dict() used to word2index 
        return [ [1 if w >= n_words else w for w in sen] for sen in x ]
    #################################################################

    test_set_x, test_set_y = test_set
    valid_set_x, valid_set_y = valid_set
    train_set_x, train_set_y = train_set

    train_set_x = remove_unk(train_set_x)
    valid_set_x = remove_unk(valid_set_x)
    test_set_x = remove_unk(test_set_x)

    #################################################################
    def len_argsort(seq):
        # seq: list of lists
        # sort seq by its elements length
        return sorted( range( len(seq) ), key=lambda x: len(seq[x])
                     )
    #################################################################

    if sort_by_len:
        sorted_index = len_argsort(test_set_x)
        test_set_x = [test_set_x[i] for i in sorted_index]
        test_set_y = [test_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(valid_set_x)
        valid_set_x = [valid_set_x[i] for i in sorted_index]
        valid_set_y = [valid_set_y[i] for i in sorted_index]

        sorted_index = len_argsort(train_set_x)
        train_set_x = [train_set_x[i] for i in sorted_index]
        train_set_y = [train_set_y[i] for i in sorted_index]

    train = (train_set_x, train_set_y)
    valid = (valid_set_x, valid_set_y)
    test = (test_set_x, test_set_y)
    
    #################################################################
    return train, valid, test


'''
####################################################################################
'''

def load_data_for_prediction(dataset, path_dataset, 
                             n_words=60000, maxlen=None, sort_by_len=True):
    '''
    Loads the dataset for prediction
    thus not test and validation set. 
    '''
    #################################################################
    # Load the dataset
    dataset = get_dataset_file(dataset=dataset, default_dataset="tweetText_tagScore.pkl", 
                               path_dataset= path_dataset) 
    print "load data: ", dataset

    if dataset.endswith(".gz"):
        f = gzip.open(dataset, 'rb')
    else:
        f = open(dataset, 'rb')

    # datafile should be created with only 1 pkl.dump()
    # and its Y values are meaningless, although with the correct max value inserted
    single_data_set = pickle.load(f)
    fileNames = pickle.load(f)
    scores_tag = pickle.load(f)

    print "check single_data_set: "
    print single_data_set[0][:3]
    print single_data_set[1][:3]    

    f.close()
    #################################################################   

    # limit sentence length by maxlen
    if maxlen:
        new_single_data_set_x = []
        new_single_data_set_y = []
        for x, y in zip(single_data_set[0], single_data_set[1]):
            if len(x) < maxlen:
                new_single_data_set_x.append(x)
                new_single_data_set_y.append(y)
        single_data_set = (new_single_data_set_x, new_single_data_set_y)
        del new_single_data_set_x, new_single_data_set_y

    #################################################################
    def remove_unk(x):
        # flag words by whether their frequency is above the threshold or not
        # threshod set by n_words and the dict() used to word2index 
        return [ [1 if w >= n_words else w for w in sen] for sen in x ]
    #################################################################

    single_data_set_x, single_data_set_y = single_data_set
    single_data_set_x = remove_unk(single_data_set_x)

    #################################################################
    def len_argsort(seq):
        # seq: list of lists
        # sort seq by its elements length
        return sorted( range( len(seq) ), key=lambda x: len(seq[x])
                     )
    #################################################################

    if sort_by_len:
        sorted_index = len_argsort(single_data_set_x)
        single_data_set_x = [single_data_set_x[i] for i in sorted_index]
        single_data_set_y = [single_data_set_y[i] for i in sorted_index]

    single_data_set = (single_data_set_x, single_data_set_y)
    
    #################################################################
    return single_data_set, fileNames, scores_tag

'''
####################################################################################
'''
